{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KwonHo-geun/automobile/blob/main/25.07.28_Pytorch%26ONNX%EB%B9%84%EA%B5%90.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "G04wQwspXms0",
        "outputId": "630f98b6-a7df-4943-a3ff-29e8fbc5a9c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ZIP íŒŒì¼ì„ ì½”ë©ìœ¼ë¡œ ë³µì‚¬\n",
        "!cp \"/content/drive/MyDrive/07_28_automobiledataset.zip\" \"/content/\"\n",
        "\n",
        "# ì••ì¶• í•´ì œ\n",
        "!unzip -o /content/07_28_automobiledataset.zip -d /content/\n",
        "\n",
        "# ì••ì¶• í•´ì œ í™•ì¸\n",
        "!ls -la /content/"
      ],
      "metadata": {
        "id": "Pj0VqbznXgLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. ultralytics ì„¤ì¹˜\n",
        "!pip install ultralytics"
      ],
      "metadata": {
        "id": "kiF72AsIZ3aY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. ì„¤ì¹˜ í™•ì¸ í›„ ë‹¤ì‹œ ì‹¤í–‰\n",
        "from ultralytics import YOLO\n",
        "import glob\n",
        "import os\n",
        "\n",
        "print(\"âœ… ultralytics ì„¤ì¹˜ ì™„ë£Œ!\")\n",
        "\n",
        "# ì´ë¯¸ í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš©\n",
        "model = YOLO('/content/dataset/best.pt')\n",
        "\n",
        "# YouTube ì˜ìƒ ë‹¤ìš´ë¡œë“œ\n",
        "!pip install yt-dlp\n",
        "!yt-dlp -f 'best[height<=720]' -o '/content/test_video.%(ext)s' 'https://www.youtube.com/watch?v=AxLmroTo3rQ'\n",
        "\n",
        "# ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ ì°¾ê¸° (í™•ì¥ìê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)\n",
        "video_files = glob.glob('/content/test_video.*')\n",
        "if video_files:\n",
        "    video_path = video_files[0]\n",
        "    print(f\"ğŸ“¹ ë‹¤ìš´ë¡œë“œëœ ì˜ìƒ: {video_path}\")\n",
        "\n",
        "    # ì¶”ë¡  ì‹¤í–‰\n",
        "    results = model(video_path)\n",
        "\n",
        "    # ê²°ê³¼ í‘œì‹œ (ì˜ìƒì˜ ê²½ìš° ì²« ë²ˆì§¸ í”„ë ˆì„ë§Œ)\n",
        "    if results:\n",
        "        results[0].show()\n",
        "else:\n",
        "    print(\"âŒ ì˜ìƒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨\")\n",
        "\n",
        "# ê¸°ì¡´ ê²€ì¦ ë°ì´í„°ë¡œ ì„±ëŠ¥ ì¸¡ì •\n",
        "print(\"\\nğŸ“Š ëª¨ë¸ ì„±ëŠ¥ í‰ê°€:\")\n",
        "metrics = model.val(data='/content/dataset/dataset.yaml')\n",
        "print(f\"mAP50: {metrics.box.map50:.4f}\")\n",
        "print(f\"mAP50-95: {metrics.box.map:.4f}\")\n",
        "print(f\"Precision: {metrics.box.mp:.4f}\")\n",
        "print(f\"Recall: {metrics.box.mr:.4f}\")\n",
        "\n",
        "print(\"\\nâœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!\")"
      ],
      "metadata": {
        "id": "9LXE0I1YXgF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "yaml íŒŒì¼ ìˆ˜ì •\n",
        "path: /content/dataset\n",
        "train: train/images\n",
        "val: valid/images\n",
        "names:\n",
        "  0: lane\n",
        "  1: traffic_sign\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "F0cCaOOehsBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. dataset.yaml íŒŒì¼ ë‚´ìš© í™•ì¸\n",
        "print(\"ğŸ“‹ dataset.yaml íŒŒì¼ ë‚´ìš©:\")\n",
        "with open('/content/dataset/dataset.yaml', 'r') as f:\n",
        "    yaml_content = f.read()\n",
        "    print(yaml_content)"
      ],
      "metadata": {
        "id": "dNYHSQGjXf_z",
        "outputId": "8492073b-fef2-48b9-ce11-62273985b462",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“‹ dataset.yaml íŒŒì¼ ë‚´ìš©:\n",
            "path: /content/dataset\n",
            "train: train/images\n",
            "val: valid/images\n",
            "names:\n",
            "  0: lane\n",
            "  1: traffic_sign\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics yt-dlp"
      ],
      "metadata": {
        "id": "RhjQKmNxaoSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import glob\n",
        "\n",
        "# yaml ìˆ˜ì • (í•µì‹¬ ë¬¸ì œ í•´ê²°)\n",
        "yaml_fix = '''path: /content/dataset\n",
        "train: train/images\n",
        "val: valid/images\n",
        "names:\n",
        "  0: lane\n",
        "  1: traffic_sign\n",
        "nc: 2'''\n",
        "\n",
        "with open('/content/dataset/dataset.yaml', 'w') as f:\n",
        "    f.write(yaml_fix)\n",
        "\n",
        "# ëª¨ë¸ ë¡œë“œ & ì˜ìƒ ë‹¤ìš´ë¡œë“œ & ì¶”ë¡ \n",
        "model = YOLO('/content/dataset/best.pt')\n",
        "!yt-dlp -f 'best[height<=720]' -o '/content/test_video.%(ext)s' 'https://www.youtube.com/watch?v=AxLmroTo3rQ'\n",
        "\n",
        "video_path = glob.glob('/content/test_video.*')[0]\n",
        "results = model(video_path)\n",
        "results[0].show()\n",
        "\n",
        "# ì„±ëŠ¥ í‰ê°€\n",
        "metrics = model.val(data='/content/dataset/dataset.yaml')\n",
        "print(f\"mAP50: {metrics.box.map50:.4f}\")"
      ],
      "metadata": {
        "id": "d_FKpd8eXf5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install ultralytics yt-dlp\n",
        "\n",
        "from ultralytics import YOLO\n",
        "import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "from IPython.display import Video\n",
        "import shutil\n",
        "import time\n",
        "\n",
        "print(\"ğŸš€ TensorRT ìµœì í™” YOLO ì¶”ë¡  ì‹œì‘!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 1ï¸âƒ£ ê¸°ë³¸ ëª¨ë¸ë“¤ ë¡œë“œ\n",
        "print(\"ğŸ¤– ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
        "base_model = YOLO('yolo11n.pt')\n",
        "custom_model = YOLO('/content/dataset/best.pt')\n",
        "\n",
        "print(f\"ê¸°ë³¸ ëª¨ë¸ í´ë˜ìŠ¤ ìˆ˜: {len(base_model.names)}\")\n",
        "print(f\"ì»¤ìŠ¤í…€ ëª¨ë¸ í´ë˜ìŠ¤ ìˆ˜: {len(custom_model.names)}\")\n",
        "\n",
        "# 2ï¸âƒ£ TensorRTë¡œ ë³€í™˜\n",
        "print(\"\\nâš¡ TensorRT ë³€í™˜ ì¤‘...\")\n",
        "print(\"ê¸°ë³¸ ëª¨ë¸ â†’ TensorRT ë³€í™˜...\")\n",
        "base_model.export(format='engine', half=True, device=0)  # FP16 ìµœì í™”\n",
        "base_trt_path = 'yolo11n.engine'\n",
        "\n",
        "print(\"ì»¤ìŠ¤í…€ ëª¨ë¸ â†’ TensorRT ë³€í™˜...\")\n",
        "custom_model.export(format='engine', half=True, device=0)\n",
        "custom_trt_path = '/content/dataset/best.engine'\n",
        "\n",
        "# 3ï¸âƒ£ TensorRT ëª¨ë¸ ë¡œë“œ\n",
        "print(\"\\nğŸ”¥ TensorRT ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
        "base_trt_model = YOLO(base_trt_path)\n",
        "custom_trt_model = YOLO(custom_trt_path)\n",
        "\n",
        "print(\"âœ… TensorRT ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\")\n",
        "\n",
        "# 4ï¸âƒ£ ì˜ìƒ ë‹¤ìš´ë¡œë“œ\n",
        "print(\"\\nğŸ“¥ YouTube ì˜ìƒ ë‹¤ìš´ë¡œë“œ ì¤‘...\")\n",
        "!yt-dlp -f 'best[height<=720]' -o '/content/test_video.%(ext)s' 'https://www.youtube.com/watch?v=AxLmroTo3rQ'\n",
        "\n",
        "video_path = glob.glob('/content/test_video.*')[0]\n",
        "print(f\"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {video_path}\")\n",
        "\n",
        "# 5ï¸âƒ£ ì„±ëŠ¥ ë¹„êµ í•¨ìˆ˜\n",
        "def performance_comparison(video_path, frames_to_test=100):\n",
        "    \"\"\"PyTorch vs TensorRT ì„±ëŠ¥ ë¹„êµ\"\"\"\n",
        "\n",
        "    print(f\"\\nâ±ï¸ ì„±ëŠ¥ ë¹„êµ (ì²« {frames_to_test}í”„ë ˆì„)\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # PyTorch ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸\n",
        "    pytorch_times = []\n",
        "    for i in range(frames_to_test):\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        start_time = time.time()\n",
        "        _ = base_model(frame, verbose=False)\n",
        "        _ = custom_model(frame, verbose=False)\n",
        "        end_time = time.time()\n",
        "\n",
        "        pytorch_times.append(end_time - start_time)\n",
        "\n",
        "    # TensorRT ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)  # ì²˜ìŒìœ¼ë¡œ ë˜ëŒë¦¬ê¸°\n",
        "    tensorrt_times = []\n",
        "    for i in range(frames_to_test):\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        start_time = time.time()\n",
        "        _ = base_trt_model(frame, verbose=False)\n",
        "        _ = custom_trt_model(frame, verbose=False)\n",
        "        end_time = time.time()\n",
        "\n",
        "        tensorrt_times.append(end_time - start_time)\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    # ê²°ê³¼ ì¶œë ¥\n",
        "    pytorch_avg = np.mean(pytorch_times) * 1000  # msë¡œ ë³€í™˜\n",
        "    tensorrt_avg = np.mean(tensorrt_times) * 1000\n",
        "    speedup = pytorch_avg / tensorrt_avg\n",
        "\n",
        "    print(f\"ğŸ PyTorch í‰ê· : {pytorch_avg:.2f}ms/frame ({1000/pytorch_avg:.1f} FPS)\")\n",
        "    print(f\"âš¡ TensorRT í‰ê· : {tensorrt_avg:.2f}ms/frame ({1000/tensorrt_avg:.1f} FPS)\")\n",
        "    print(f\"ğŸš€ ì†ë„ í–¥ìƒ: {speedup:.2f}x\")\n",
        "\n",
        "    return speedup\n",
        "\n",
        "# ì„±ëŠ¥ ë¹„êµ ì‹¤í–‰\n",
        "speedup_ratio = performance_comparison(video_path)\n",
        "\n",
        "# 6ï¸âƒ£ TensorRT ìµœì í™”ëœ ê²°í•© ì¶”ë¡ \n",
        "def tensorrt_combined_inference(video_path, output_path='/content/tensorrt_result.mp4'):\n",
        "    \"\"\"TensorRT ìµœì í™”ëœ ê²°í•© ì¶”ë¡ \"\"\"\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # ì¶œë ¥ ì˜ìƒ ì„¤ì •\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    print(f\"\\nğŸ¬ TensorRT ìµœì í™” ì˜ìƒ ì²˜ë¦¬ ì¤‘... (ì´ {total_frames} í”„ë ˆì„)\")\n",
        "\n",
        "    frame_count = 0\n",
        "    total_inference_time = 0\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # TensorRT ì¶”ë¡  (ì‹œê°„ ì¸¡ì •)\n",
        "        start_time = time.time()\n",
        "\n",
        "        # ê¸°ë³¸ TensorRT ëª¨ë¸ ì¶”ë¡ \n",
        "        base_results = base_trt_model(frame, verbose=False)\n",
        "\n",
        "        # ì»¤ìŠ¤í…€ TensorRT ëª¨ë¸ ì¶”ë¡ \n",
        "        custom_results = custom_trt_model(frame, verbose=False)\n",
        "\n",
        "        inference_time = time.time() - start_time\n",
        "        total_inference_time += inference_time\n",
        "\n",
        "        # ê²°ê³¼ ì‹œê°í™”\n",
        "        annotated_frame = frame.copy()\n",
        "\n",
        "        # ê¸°ë³¸ YOLO ê²°ê³¼ ê·¸ë¦¬ê¸° (íŒŒë€ìƒ‰)\n",
        "        if base_results[0].boxes is not None:\n",
        "            for box in base_results[0].boxes:\n",
        "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "                conf = float(box.conf[0])\n",
        "                cls = int(box.cls[0])\n",
        "\n",
        "                if conf > 0.3:\n",
        "                    label = f\"{base_trt_model.names[cls]} {conf:.2f}\"\n",
        "                    cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "                    cv2.putText(annotated_frame, label, (x1, y1-10),\n",
        "                              cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
        "\n",
        "        # ì»¤ìŠ¤í…€ YOLO ê²°ê³¼ ê·¸ë¦¬ê¸° (ë¹¨ê°„ìƒ‰)\n",
        "        if custom_results[0].boxes is not None:\n",
        "            for box in custom_results[0].boxes:\n",
        "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "                conf = float(box.conf[0])\n",
        "                cls = int(box.cls[0])\n",
        "\n",
        "                if conf > 0.3:\n",
        "                    label = f\"{custom_trt_model.names[cls]} {conf:.2f}\"\n",
        "                    cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
        "                    cv2.putText(annotated_frame, label, (x1, y1-10),\n",
        "                              cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
        "\n",
        "        # TensorRT ì •ë³´ í‘œì‹œ\n",
        "        fps_text = f\"TensorRT: {1/inference_time:.1f} FPS\"\n",
        "        cv2.putText(annotated_frame, fps_text, (10, 30),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "\n",
        "        out.write(annotated_frame)\n",
        "        frame_count += 1\n",
        "\n",
        "        if frame_count % 50 == 0:\n",
        "            avg_fps = frame_count / total_inference_time\n",
        "            print(f\"   ì²˜ë¦¬ ì¤‘... {frame_count}/{total_frames} ({frame_count/total_frames*100:.1f}%) - í‰ê·  {avg_fps:.1f} FPS\")\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    avg_fps = frame_count / total_inference_time\n",
        "    print(f\"âœ… TensorRT ê²°ê³¼ ì˜ìƒ ì €ì¥: {output_path}\")\n",
        "    print(f\"ğŸ“Š í‰ê·  ì²˜ë¦¬ ì†ë„: {avg_fps:.1f} FPS\")\n",
        "\n",
        "    return avg_fps\n",
        "\n",
        "# 7ï¸âƒ£ TensorRT ìµœì í™”ëœ ì¶”ë¡  ì‹¤í–‰\n",
        "print(\"\\nğŸ”¥ TensorRT ìµœì í™”ëœ ê²°í•© ì¶”ë¡  ì‹¤í–‰...\")\n",
        "tensorrt_fps = tensorrt_combined_inference(video_path, '/content/tensorrt_final_result.mp4')\n",
        "\n",
        "# 8ï¸âƒ£ ê¸°ì¡´ PyTorch ì¶”ë¡ ë„ ì‹¤í–‰ (ë¹„êµìš©)\n",
        "print(\"\\nğŸ PyTorch ê¸°ì¡´ ì¶”ë¡  (ë¹„êµìš©)...\")\n",
        "def pytorch_combined_inference(video_path, output_path='/content/pytorch_result.mp4'):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    frame_count = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        base_results = base_model(frame, verbose=False)\n",
        "        custom_results = custom_model(frame, verbose=False)\n",
        "\n",
        "        # ê°„ë‹¨í•œ ì‹œê°í™” (ì†ë„ ë¹„êµìš©)\n",
        "        annotated_frame = frame.copy()\n",
        "        cv2.putText(annotated_frame, \"PyTorch\", (10, 30),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2)\n",
        "\n",
        "        out.write(annotated_frame)\n",
        "        frame_count += 1\n",
        "\n",
        "        if frame_count >= 100:  # 100í”„ë ˆì„ë§Œ ì²˜ë¦¬ (ë¹„êµìš©)\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    pytorch_fps = frame_count / total_time\n",
        "    return pytorch_fps\n",
        "\n",
        "pytorch_fps = pytorch_combined_inference(video_path)\n",
        "\n",
        "# 9ï¸âƒ£ ì„±ëŠ¥ í‰ê°€ (ì»¤ìŠ¤í…€ ëª¨ë¸)\n",
        "print(\"\\nğŸ“Š ì»¤ìŠ¤í…€ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€:\")\n",
        "metrics = custom_model.val(data='/content/dataset/dataset.yaml')  # custom_trt_model ëŒ€ì‹  custom_model\n",
        "print(f\"mAP50: {metrics.box.map50:.4f}\")\n",
        "\n",
        "# ğŸ”Ÿ ìµœì¢… ê²°ê³¼ ë° ë¹„êµ\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ¯ ìµœì¢… ì„±ëŠ¥ ë¹„êµ ê²°ê³¼:\")\n",
        "print(f\"ğŸ PyTorch: {pytorch_fps:.1f} FPS\")\n",
        "print(f\"âš¡ TensorRT: {tensorrt_fps:.1f} FPS\")\n",
        "print(f\"ğŸš€ ì „ì²´ ì†ë„ í–¥ìƒ: {tensorrt_fps/pytorch_fps:.2f}x\")\n",
        "\n",
        "print(f\"\\nğŸ“Š ëª¨ë¸ ì •í™•ë„ (mAP50): {metrics.box.map50:.4f}\")\n",
        "\n",
        "print(\"\\nğŸ¬ ìµœì¢… TensorRT ê²°ê³¼ ì˜ìƒ:\")\n",
        "Video('/content/tensorrt_final_result.mp4', width=800)\n",
        "\n",
        "print(\"\\nğŸ‰ TensorRT ìµœì í™” ì™„ë£Œ!\")\n",
        "print(\"ğŸ”µ íŒŒë€ìƒ‰ ë°•ìŠ¤: ê¸°ë³¸ YOLO ê°ì²´ë“¤ (TensorRT ìµœì í™”)\")\n",
        "print(\"ğŸ”´ ë¹¨ê°„ìƒ‰ ë°•ìŠ¤: ì»¤ìŠ¤í…€ ê°ì²´ë“¤ (TensorRT ìµœì í™”)\")\n",
        "print(\"ğŸ’š ì´ˆë¡ìƒ‰ í…ìŠ¤íŠ¸: ì‹¤ì‹œê°„ FPS í‘œì‹œ\")\n",
        "\n",
        "print(\"\\nğŸ’¾ ìƒì„±ëœ íŒŒì¼ë“¤:\")\n",
        "print(\"- tensorrt_final_result.mp4: TensorRT ìµœì í™”ëœ ìµœì¢… ê²°ê³¼\")\n",
        "print(\"- pytorch_result.mp4: PyTorch ë¹„êµìš© ê²°ê³¼\")\n",
        "print(\"- yolo11n.engine: ê¸°ë³¸ ëª¨ë¸ TensorRT ì—”ì§„\")\n",
        "print(\"- best.engine: ì»¤ìŠ¤í…€ ëª¨ë¸ TensorRT ì—”ì§„\")"
      ],
      "metadata": {
        "id": "PcDhpQri0lOZ",
        "outputId": "a7c4a6b1-0e8a-4382-c5a6-a634b62136ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ TensorRT ìµœì í™” YOLO ì¶”ë¡  ì‹œì‘!\n",
            "============================================================\n",
            "ğŸ¤– ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ ì¤‘...\n",
            "ê¸°ë³¸ ëª¨ë¸ í´ë˜ìŠ¤ ìˆ˜: 80\n",
            "ì»¤ìŠ¤í…€ ëª¨ë¸ í´ë˜ìŠ¤ ìˆ˜: 2\n",
            "\n",
            "âš¡ TensorRT ë³€í™˜ ì¤‘...\n",
            "ê¸°ë³¸ ëª¨ë¸ â†’ TensorRT ë³€í™˜...\n",
            "Ultralytics 8.3.170 ğŸš€ Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients, 6.5 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolo11n.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (5.4 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.61...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 1.4s, saved as 'yolo11n.onnx' (10.2 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m starting export with TensorRT 10.13.0.35...\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m input \"images\" with shape(1, 3, 640, 640) DataType.FLOAT\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m output \"output0\" with shape(1, 84, 8400) DataType.FLOAT\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m building FP16 engine as yolo11n.engine\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m export success âœ… 230.9s, saved as 'yolo11n.engine' (8.9 MB)\n",
            "\n",
            "Export complete (231.2s)\n",
            "Results saved to \u001b[1m/content\u001b[0m\n",
            "Predict:         yolo predict task=detect model=yolo11n.engine imgsz=640 half \n",
            "Validate:        yolo val task=detect model=yolo11n.engine imgsz=640 data=/usr/src/ultralytics/ultralytics/cfg/datasets/coco.yaml half \n",
            "Visualize:       https://netron.app\n",
            "ì»¤ìŠ¤í…€ ëª¨ë¸ â†’ TensorRT ë³€í™˜...\n",
            "Ultralytics 8.3.170 ğŸš€ Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 3,006,038 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '/content/dataset/best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 6, 8400) (6.0 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.61...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 1.9s, saved as '/content/dataset/best.onnx' (11.6 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m starting export with TensorRT 10.13.0.35...\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m input \"images\" with shape(1, 3, 640, 640) DataType.FLOAT\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m output \"output0\" with shape(1, 6, 8400) DataType.FLOAT\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m building FP16 engine as /content/dataset/best.engine\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m export success âœ… 184.7s, saved as '/content/dataset/best.engine' (8.9 MB)\n",
            "\n",
            "Export complete (185.0s)\n",
            "Results saved to \u001b[1m/content/dataset\u001b[0m\n",
            "Predict:         yolo predict task=detect model=/content/dataset/best.engine imgsz=640 half \n",
            "Validate:        yolo val task=detect model=/content/dataset/best.engine imgsz=640 data=/content/dataset/dataset.yaml half \n",
            "Visualize:       https://netron.app\n",
            "\n",
            "ğŸ”¥ TensorRT ëª¨ë¸ ë¡œë“œ ì¤‘...\n",
            "WARNING âš ï¸ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
            "WARNING âš ï¸ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
            "âœ… TensorRT ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\n",
            "\n",
            "ğŸ“¥ YouTube ì˜ìƒ ë‹¤ìš´ë¡œë“œ ì¤‘...\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=AxLmroTo3rQ\n",
            "[youtube] AxLmroTo3rQ: Downloading webpage\n",
            "[youtube] AxLmroTo3rQ: Downloading tv client config\n",
            "[youtube] AxLmroTo3rQ: Downloading tv player API JSON\n",
            "[youtube] AxLmroTo3rQ: Downloading ios player API JSON\n",
            "[youtube] AxLmroTo3rQ: Downloading m3u8 information\n",
            "[info] AxLmroTo3rQ: Downloading 1 format(s): 18\n",
            "[download] /content/test_video.mp4 has already been downloaded\n",
            "\u001b[K[download] 100% of    7.14MiB\n",
            "âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: /content/test_video.mp4\n",
            "\n",
            "â±ï¸ ì„±ëŠ¥ ë¹„êµ (ì²« 100í”„ë ˆì„)\n",
            "--------------------------------------------------\n",
            "Loading yolo11n.engine for TensorRT inference...\n",
            "Loading /content/dataset/best.engine for TensorRT inference...\n",
            "ğŸ PyTorch í‰ê· : 24.25ms/frame (41.2 FPS)\n",
            "âš¡ TensorRT í‰ê· : 12.30ms/frame (81.3 FPS)\n",
            "ğŸš€ ì†ë„ í–¥ìƒ: 1.97x\n",
            "\n",
            "ğŸ”¥ TensorRT ìµœì í™”ëœ ê²°í•© ì¶”ë¡  ì‹¤í–‰...\n",
            "\n",
            "ğŸ¬ TensorRT ìµœì í™” ì˜ìƒ ì²˜ë¦¬ ì¤‘... (ì´ 3990 í”„ë ˆì„)\n",
            "   ì²˜ë¦¬ ì¤‘... 50/3990 (1.3%) - í‰ê·  91.0 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 100/3990 (2.5%) - í‰ê·  91.0 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 150/3990 (3.8%) - í‰ê·  88.5 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 200/3990 (5.0%) - í‰ê·  86.3 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 250/3990 (6.3%) - í‰ê·  81.7 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 300/3990 (7.5%) - í‰ê·  78.6 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 350/3990 (8.8%) - í‰ê·  76.2 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 400/3990 (10.0%) - í‰ê·  75.8 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 450/3990 (11.3%) - í‰ê·  75.5 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 500/3990 (12.5%) - í‰ê·  75.4 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 550/3990 (13.8%) - í‰ê·  75.3 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 600/3990 (15.0%) - í‰ê·  75.2 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 650/3990 (16.3%) - í‰ê·  75.1 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 700/3990 (17.5%) - í‰ê·  75.1 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 750/3990 (18.8%) - í‰ê·  75.1 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 800/3990 (20.1%) - í‰ê·  75.1 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 850/3990 (21.3%) - í‰ê·  75.2 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 900/3990 (22.6%) - í‰ê·  75.2 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 950/3990 (23.8%) - í‰ê·  74.7 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 1000/3990 (25.1%) - í‰ê·  74.1 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 1050/3990 (26.3%) - í‰ê·  73.6 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 1100/3990 (27.6%) - í‰ê·  73.8 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 1150/3990 (28.8%) - í‰ê·  74.0 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 1200/3990 (30.1%) - í‰ê·  74.3 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 1250/3990 (31.3%) - í‰ê·  74.5 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 1300/3990 (32.6%) - í‰ê·  74.7 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 1350/3990 (33.8%) - í‰ê·  74.9 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 1400/3990 (35.1%) - í‰ê·  75.1 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 1450/3990 (36.3%) - í‰ê·  75.3 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 1500/3990 (37.6%) - í‰ê·  75.5 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 1550/3990 (38.8%) - í‰ê·  75.6 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 1600/3990 (40.1%) - í‰ê·  75.8 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 1650/3990 (41.4%) - í‰ê·  75.9 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 1700/3990 (42.6%) - í‰ê·  75.6 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 1750/3990 (43.9%) - í‰ê·  75.2 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 1800/3990 (45.1%) - í‰ê·  74.8 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 1850/3990 (46.4%) - í‰ê·  74.8 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 1900/3990 (47.6%) - í‰ê·  74.9 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 1950/3990 (48.9%) - í‰ê·  75.0 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 2000/3990 (50.1%) - í‰ê·  75.1 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 2050/3990 (51.4%) - í‰ê·  75.4 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 2100/3990 (52.6%) - í‰ê·  75.6 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 2150/3990 (53.9%) - í‰ê·  75.8 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 2200/3990 (55.1%) - í‰ê·  76.0 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 2250/3990 (56.4%) - í‰ê·  76.1 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 2300/3990 (57.6%) - í‰ê·  76.1 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 2350/3990 (58.9%) - í‰ê·  76.2 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 2400/3990 (60.2%) - í‰ê·  76.2 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 2450/3990 (61.4%) - í‰ê·  75.9 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 2500/3990 (62.7%) - í‰ê·  75.6 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 2550/3990 (63.9%) - í‰ê·  75.3 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 2600/3990 (65.2%) - í‰ê·  75.3 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 2650/3990 (66.4%) - í‰ê·  75.3 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 2700/3990 (67.7%) - í‰ê·  75.3 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 2750/3990 (68.9%) - í‰ê·  75.3 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 2800/3990 (70.2%) - í‰ê·  75.3 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 2850/3990 (71.4%) - í‰ê·  75.3 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 2900/3990 (72.7%) - í‰ê·  75.3 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 2950/3990 (73.9%) - í‰ê·  75.4 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 3000/3990 (75.2%) - í‰ê·  75.4 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 3050/3990 (76.4%) - í‰ê·  75.4 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 3100/3990 (77.7%) - í‰ê·  75.4 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 3150/3990 (78.9%) - í‰ê·  75.2 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 3200/3990 (80.2%) - í‰ê·  75.0 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 3250/3990 (81.5%) - í‰ê·  74.8 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 3300/3990 (82.7%) - í‰ê·  74.8 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 3350/3990 (84.0%) - í‰ê·  74.8 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 3400/3990 (85.2%) - í‰ê·  74.8 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 3450/3990 (86.5%) - í‰ê·  74.8 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 3500/3990 (87.7%) - í‰ê·  74.8 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 3550/3990 (89.0%) - í‰ê·  74.7 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 3600/3990 (90.2%) - í‰ê·  74.7 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 3650/3990 (91.5%) - í‰ê·  74.7 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 3700/3990 (92.7%) - í‰ê·  74.7 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 3750/3990 (94.0%) - í‰ê·  74.7 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 3800/3990 (95.2%) - í‰ê·  74.5 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 3850/3990 (96.5%) - í‰ê·  74.3 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 3900/3990 (97.7%) - í‰ê·  74.2 FPS\n",
            "   ì²˜ë¦¬ ì¤‘... 3950/3990 (99.0%) - í‰ê·  74.3 FPS\n",
            "âœ… TensorRT ê²°ê³¼ ì˜ìƒ ì €ì¥: /content/tensorrt_final_result.mp4\n",
            "ğŸ“Š í‰ê·  ì²˜ë¦¬ ì†ë„: 74.3 FPS\n",
            "\n",
            "ğŸ PyTorch ê¸°ì¡´ ì¶”ë¡  (ë¹„êµìš©)...\n",
            "\n",
            "ğŸ“Š ì»¤ìŠ¤í…€ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€:\n",
            "Ultralytics 8.3.170 ğŸš€ Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1794.9Â±181.2 MB/s, size: 301.2 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/dataset/valid/labels.cache... 72 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 72/72 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:02<00:00,  2.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         72        569      0.614      0.486      0.478      0.192\n",
            "                  lane         72        497      0.529      0.416      0.433      0.142\n",
            "          traffic_sign         34         72      0.699      0.556      0.523      0.243\n",
            "Speed: 4.8ms preprocess, 3.6ms inference, 0.0ms loss, 4.5ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/val4\u001b[0m\n",
            "mAP50: 0.4781\n",
            "\n",
            "============================================================\n",
            "ğŸ¯ ìµœì¢… ì„±ëŠ¥ ë¹„êµ ê²°ê³¼:\n",
            "ğŸ PyTorch: 41.5 FPS\n",
            "âš¡ TensorRT: 74.3 FPS\n",
            "ğŸš€ ì „ì²´ ì†ë„ í–¥ìƒ: 1.79x\n",
            "\n",
            "ğŸ“Š ëª¨ë¸ ì •í™•ë„ (mAP50): 0.4781\n",
            "\n",
            "ğŸ¬ ìµœì¢… TensorRT ê²°ê³¼ ì˜ìƒ:\n",
            "\n",
            "ğŸ‰ TensorRT ìµœì í™” ì™„ë£Œ!\n",
            "ğŸ”µ íŒŒë€ìƒ‰ ë°•ìŠ¤: ê¸°ë³¸ YOLO ê°ì²´ë“¤ (TensorRT ìµœì í™”)\n",
            "ğŸ”´ ë¹¨ê°„ìƒ‰ ë°•ìŠ¤: ì»¤ìŠ¤í…€ ê°ì²´ë“¤ (TensorRT ìµœì í™”)\n",
            "ğŸ’š ì´ˆë¡ìƒ‰ í…ìŠ¤íŠ¸: ì‹¤ì‹œê°„ FPS í‘œì‹œ\n",
            "\n",
            "ğŸ’¾ ìƒì„±ëœ íŒŒì¼ë“¤:\n",
            "- tensorrt_final_result.mp4: TensorRT ìµœì í™”ëœ ìµœì¢… ê²°ê³¼\n",
            "- pytorch_result.mp4: PyTorch ë¹„êµìš© ê²°ê³¼\n",
            "- yolo11n.engine: ê¸°ë³¸ ëª¨ë¸ TensorRT ì—”ì§„\n",
            "- best.engine: ì»¤ìŠ¤í…€ ëª¨ë¸ TensorRT ì—”ì§„\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Colab ì‹œì‘í•˜ê¸°",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
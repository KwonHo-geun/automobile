# YOLO(Object Detection) 소개

안녕하세요! 이 문서는 **YOLO(You Only Look Once)** 객체 탐지 알고리즘의 기본 개념, 작동 원리, 최신 버전별 특징, 장단점, 활용 예시를 한눈에 이해할 수 있도록 정리한 페이지입니다.

## 1. YOLO란 무엇인가?

**YOLO**는 이미지를 한 번만 신경망에 통과시켜 객체의 위치와 클래스를 동시에 예측하는 실시간 딥러닝 기반 객체 탐지(Object Detection) 알고리즘입니다.

- **실시간 동작**: 이미지 분석 및 예측이 빠르기 때문에 비디오, 로봇, 자율주행 등 실시간 환경에 적합합니다.
- **단일 네트워크**: 이미지 전체를 한 번에 처리하여 엔드-투-엔드(End-to-End) 구조를 가집니다.

## 2. YOLO의 작동 원리

1. **이미지 그리드 분할(Grid division)**  
   - 입력 이미지를 SxS 그리드로 나눕니다.
2. **박스 및 신뢰도 예측(Bounding Box & Confidence)**  
   - 각 그리드 셀마다 여러 박스 좌표와 신뢰도 점수를 예측합니다.
3. **클래스 확률 예측(Class probabilities)**  
   - 각 박스의 클래스별 확률값을 계산합니다.
4. **최종 탐지 결과 생성**  
   - 박스와 클래스 확률을 조합, NMS 기법을 이용해 중복 탐지 제거 후 결과 출력

## 3. YOLO 주요 버전별 특징 (v8 ~ v12)

| 버전    | 주요 특징                                 | 장점                             | 단점                              |
|---------|-------------------------------------------|----------------------------------|-----------------------------------|
| YOLOv8  | 최신 실용적 아키텍처, 모듈화, 유연성 강화 | 정확도·속도 향상, 경량화, 쉬운 확장 | 하위 버전과의 일부 호환성 문제     |
| YOLOv9  | 효율적 구조, 동적 헤드, 학습 전략 최적화   | 더 향상된 탐지력, 더 적은 연산비용 | 구현 복잡도 증가                  |
| YOLOv10 | SOTA(최첨단) 성능, 하드웨어 최적화         | 하드웨어별 최적 실행, 뛰어난 실시간성 | 구현 환경 제한 가능성             |
| YOLOv11 | 멀티태스킹 통합, 새로운 특성 인코딩 기법   | 다양한 태스크 동시 처리, 확장성   | 복잡성 증가, 실제 적용 시 커스터마이징 필요성 |
| YOLOv12 | 최첨단 데이터 증강 및 적응적 학습 적용    | 뛰어난 일반화, 준지도·자체지도 강화 | 최신 하드웨어 필요, 실험적 기능 존재 |

## 4. 장점 및 한계

### 장점

- 매우 빠른 처리 속도 (실시간 적용 가능)
- 다양한 기기에서 활용 가능 (경량화)
- 엔드-투-엔드 구조로 구현과 사용이 단순함
- 버전이 올라갈수록 정확도와 활용성 지속 향상

### 한계

- 작은 객체, 특히 그리드 경계에 걸친 객체 탐지 성능이 낮을 수 있음
- 최신 버전일수록 실제 환경 적용의 복잡도가 증가함
- 하위 버전과의 완전한 호환이 어려운 경우가 있음

# YOLOv11, YOLOv12 상세 정리

## YOLOv11란?

YOLOv11은 Ultralytics 시리즈의 최신 실시간 객체 탐지(오브젝트 디텍션) 모델로, 이전 버전들과 비교해 **정확도, 속도, 효율성**이 모두 크게 향상되었습니다. 다양한 비전 작업에 적용할 수 있도록 유연하고, 엣지/클라우드/다양한 GPU 환경에서 즉시 사용이 가능합니다[1][2][3][7][4].

### YOLOv11의 주요 특징

- **향상된 백본 및 넥 구조:**  
  - 새로운 C3k2, SPPF, C2PSA 블록 도입으로 더 깊고 효과적인 특징(Feature) 추출이 가능해져, 복잡한 객체 탐지에도 높은 성능 발휘[2][3].
- **효율성과 속도 개선:**  
  - 최적화된 구조와 트레이닝 파이프라인으로 동급 모델 대비 더 빠른 처리 속도와 높은 mAP(정확도) 달성.
  - YOLOv8m 대비 파라미터를 약 22% 줄였으나 정확도는 오히려 향상됨[1][3].
- **다양한 컴퓨터 비전 작업 지원:**  
  - 객체 탐지, 인스턴스 세분화, 이미지 분류, 자세 추정, 지향성 바운딩 박스(OBB), 객체 추적 등 다양한 태스크 지원[2][5][3].
- **멀티사이즈 및 확장성:**  
  - 경량(n, s)부터 대형(x)까지 다양한 모델 제공. 대형(X) 모델 기준 COCO mAP 54% 이상[2][3].
- **실시간성과 낮은 지연률:**  
  - 고해상도 입력, 저지연 실시간 환경에도 적합. 낮은 파라미터 수로 엣지 기기에서도 뛰어난 처리 속도를 보장.
- **적용 분야 확장:**  
  - 자율주행, 헬스케어, 로보틱스, 보안/감시, 산업 자동화 등 실제 산업 전반에 활용[1][7][15].

### YOLOv11 아키텍처 요약

| 주요 구성 요소            | 내용                                                         |
|-------------------------|-------------------------------------------------------------|
| 백본(Backbone)           | C3k2 구조, Spacial Pyramid Pooling-Fast(SPPF), 효율적 특성 추출   |
| 넥(Neck)                | 다중 스케일 특성 통합, Parallel Spatial Attention 도입             |
| 헤드(Head)              | 객체 탐지, 분류, 위치 추정 등 다양한 출력 담당                 |
| 지원 전처리              | Augmentation, Data sampling 등 최신 트릭 지원                   |
| 지원 태스크              | Detection, Segmentation, Classification, Pose, OBB, Tracking |

## YOLOv12란?

YOLOv12는 **Attention-centric architecture**를 도입한 최신 YOLO 모델로, 실시간(object detection) 분야에서 정확도와 속도 면에서 SOTA(State-of-the-Art)를 달성하면서도 다양한 디바이스에 최적화된 설계를 보여줍니다[6][10][8][14][12].

### YOLOv12의 주요 특징

- **Attention 중심 구조 적용:**  
  - 기존 CNN 방식에서 벗어나 Area Attention, FlashAttention 등 최첨단 Attention 구조를 도입, 잡음 환경/중복 객체/작은 객체에서 높은 정확도를 보장[10][8][14].
- **R-ELAN(Residual Efficient Layer Aggregation Network):**  
  - 레이어간 정보 재조합과 잔차 연결로 깊은 신경망에서도 효율적으로 특성 융합 및 학습 가능, 최적화 및 정밀도 동시 확보[8][10].
- **7x7 Separable Convolution & Position Perceiver:**  
  - 공간적 정보를 보존하며 대규모 이미지에서도 효율적 특성 추출.
- **최적화된 Neck 및 Head:**  
  - 특성 통합력이 높고, 다양한 스케일·시나리오에 대한 적응성 제고.
- **경량화와 속도 동시 달성:**  
  - YOLOv11m 대비 최대 25% 파라미터 감소, 더 작지만 뛰어난 정확도 유지[6][10][8].
  - 엣지, 임베디드 환경에서 실시간 처리 가능.
- **고도화된 학습 파이프라인:**  
  - Mosaic, MixUp 등 최신 데이터증강, dynamic learning rate 등으로 더욱 견고한 성능 제공.
- **인스턴스 세분화, 분류, 포즈, OBB 등 멀티태스크 완벽 지원.**

### YOLOv12 아키텍처 요약

| 주요 구성 요소      | 내용                                                         |
|-------------------|-------------------------------------------------------------|
| 백본(Backbone)     | R-ELAN, 7x7 Separable Conv, Multi-scale Feature Pyramid      |
| 넥(Neck)          | Area Attention, FlashAttention, depthwise separable layer    |
| 헤드(Head)        | 대형 receptive field, refined regression, nonlinear activation|
| 파라미터 최적화    | 경량 블록, 프루닝 및 양자화(Quantization)로 최적화, 다양한 하드웨어 대응  |
| 지원 태스크        | Detection, Instance Segmentation, Classification, Pose, OBB 등|

### YOLOv12의 대표적 혁신

- **Area Attention:**  
  이미지를 구역별로 주의(attention)를 할당, 작은 객체·밀집 객체 탐지에 특화됨.
- **FlashAttention:**  
  메모리 접근 최적화 및 고속 attention 연산, 실시간 대용량 데이터에 특히 강점.
- **R-ELAN:**  
  기존 ELAN구조에 Residual connection 및 확장형 feature aggregation으로 깊이가 깊어질수록 성능 저하 없이 효율 개선.
- **실제 사례:**  
  자율주행, 재난 상황 감시, 항공영상 분석, 스마트 팩토리, 의료 영상 등 다양한 현실 환경에서 실제 적용 속도가 빠름.

## 요약비교: YOLOv11 vs YOLOv12

| 항목        | YOLOv11                                                                          | YOLOv12                                                                          |
|-------------|----------------------------------------------------------------------------------|----------------------------------------------------------------------------------|
| 백본        | C3k2, SPPF, C2PSA 등 신형 CNN 블록                   | R-ELAN, 7x7 Separable Conv, Area Attention           |
| attention   | Parallel Spatial Attention(PSA) 포함                 | Area Attention, FlashAttention 등 혁신 Attention 구조 |
| 특징        | 정확성 향상·경량화·다태스크(분류, 세분화 등) 지원              | 작은·중첩 객체 극강 정확도, 다중 태스크 완벽 지원, 신속 최적화         |
| 파라미터    | v8 대비 최대 22%↓(동급 정확도)                                   | v11 대비 최대 25%↓(동급 정확도)                                   |
| 지원 분야   | 탐지, 분류, 세분화, 자세, OBB, 추적 등                             | 탐지, 세분화, 분류, 자세, OBB, 실시간 임베디드/엣지 등               |

